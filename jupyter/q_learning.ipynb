{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo.dynamicProgramming import dynamicPlayer\n",
    "from algo.iplayer import RandomPlayer, IPlayer\n",
    "from algo.q_learning import QLearning\n",
    "from algo.board import Board, GameState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    # \"cuda\" if torch.cuda.is_available() else\n",
    "    # \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tUsing structure similar to NNUE:\n",
    "\thttps://www.chessprogramming.org/File:StockfishNNUELayers.png\n",
    "\t\n",
    "\tObservation space: hot-encoded board:\n",
    "\tfor each of 18 cells we can be -2, -1, 0, 1, 2 (5 possibilities).\n",
    "\tIn total it gives 18 * 5 = 90 possible inputs, out of which at most 12 are on.\n",
    "\t\n",
    "\t# Action space: 4 possible actions.\n",
    "\tValue function: 1 output. # https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DQN, self).__init__()\n",
    "\n",
    "\t\tlayer_sizes = [\n",
    "\t\t\t90,\n",
    "\t\t\t50,\n",
    "\t\t\t50,\n",
    "\t\t\t50,\n",
    "\t\t\t1\n",
    "\t\t]\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tprev_size = layer_sizes[0]\n",
    "\t\tfor cur_size in layer_sizes[1:]:\n",
    "\t\t\tlayer = nn.Linear(prev_size, cur_size)\n",
    "\t\t\tnn.init.kaiming_uniform(layer.weight, nonlinearity='relu')\n",
    "\t\t\tlayers.append(layer)\n",
    "\t\t\tprev_size = cur_size\n",
    "\n",
    "\t\tself.layers = nn.ModuleList(layers)\n",
    "\n",
    "\tdef forward(self, board: Board) -> torch.Tensor:\n",
    "\t\tstate = board.to_tensor(device)\n",
    "\t\tfor layer in self.layers[:-1]:\n",
    "\t\t\tstate = F.relu(layer(state))\n",
    "\t\treturn self.layers[-1](state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment_step(state: Board, action: tuple[tuple[int, int], tuple[int, int]], enemy: IPlayer, current_step: int) -> tuple[Board, torch.Tensor]:\n",
    "\t\"\"\"\n",
    "\tReturns new state and reward for the given action.\n",
    "\t\"\"\"\n",
    "\tstate = copy.deepcopy(state)\n",
    "\tcur_sign = state.turn_sign\n",
    "\twe_captured = state.make_move(*action) * cur_sign\n",
    "\tenemy_captured = 0\n",
    "\twhile state.game_state == GameState.NOT_OVER and state.turn_sign != cur_sign:\n",
    "\t\tenemy_captured += state.make_move(*enemy.decide_move(state)) * cur_sign * (-1)\n",
    "\n",
    "\treward = we_captured - enemy_captured\n",
    "\tif current_step > 10 and not we_captured:\n",
    "\t\treward -= 2\n",
    "\t\t\n",
    "\tif state.game_state != GameState.NOT_OVER:\n",
    "\t\tour_pieces = 0\n",
    "\t\tenemy_pieces = 0\n",
    "\t\tfor _, piece in state:\n",
    "\t\t\tif piece == cur_sign:\n",
    "\t\t\t\tour_pieces += 1\n",
    "\t\t\telif piece == -cur_sign:\n",
    "\t\t\t\tenemy_pieces += 1\n",
    "\t\t\telif piece == 2 * cur_sign:\n",
    "\t\t\t\tour_pieces += 2\n",
    "\t\t\telif piece == -2 * cur_sign:\n",
    "\t\t\t\tenemy_pieces += 2\n",
    "\t\t\t\n",
    "\t\treward += 3 * our_pieces / (enemy_pieces + 1)\n",
    "\n",
    "\t\tif state.game_state == GameState.DRAW:\n",
    "\t\t\treward -= 40\n",
    "\t\telif state.game_state == GameState(cur_sign):\n",
    "\t\t\treward += 40\n",
    "\t\telif state.game_state == GameState(-cur_sign):\n",
    "\t\t\treward -= 40\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Unexpected game state\")\n",
    "\t\n",
    "\treturn state, torch.Tensor([reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99 # discount rate\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "\taction: tuple[tuple[int, int], tuple[int, int]]\n",
    "\tvalue: torch.Tensor\n",
    "\n",
    "def q_s(dqn: DQN, current_state: Board) -> list[Action]:\n",
    "\t\"\"\"\n",
    "\tReturn: list[(new_state, action, immediate_reward, value)]\n",
    "\t\"\"\"\n",
    "\tret: list[Action] = []\n",
    "\tfor s in current_state.get_possible_pos():\n",
    "\t\tfor e in current_state.get_correct_moves(s):\n",
    "\t\t\tnext_state = copy.deepcopy(current_state)\n",
    "\t\t\timmediate_reward = torch.tensor([next_state.make_move(s, e) * next_state.turn_sign], device=device)\n",
    "\t\t\tvalue = dqn(next_state) * GAMMA + immediate_reward\n",
    "\t\t\tret.append(Action((s, e), value))\n",
    "\treturn ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_304692\\3302856370.py:29: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  nn.init.kaiming_uniform(layer.weight, nonlinearity='relu')\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 # number of transitions sampled from the replay buffer\n",
    "\n",
    "EPS_START = 0.8 # exploration rate\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005 # update rate\n",
    "LR = 1e-4 # AdamW learning rate\n",
    "\n",
    "policy_net = DQN().to(device) # to be updated often\n",
    "target_net = DQN().to(device) # to be updated with TAU\n",
    "# policy_net.load_state_dict(torch.load(\"dqn80.pth\"))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(board: Board) -> Action:\n",
    "\tglobal steps_done\n",
    "\tsample = random.random()\n",
    "\teps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "\t\tmath.exp(-1. * steps_done / EPS_DECAY)\n",
    "\tsteps_done += 1\n",
    "\tif sample > eps_threshold:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate_values = q_s(policy_net, board)\n",
    "\t\t\treturn max(state_values, key=lambda x: x.value.item())\n",
    "\t\t\t\t\t\n",
    "\telse:\n",
    "\t\tpossible_moves = []\n",
    "\t\tfor s in board.get_possible_pos():\n",
    "\t\t\tfor e in board.get_correct_moves(s):\n",
    "\t\t\t\tpossible_moves.append((s, e))\n",
    "\t\treturn Action(\n",
    "\t\t\trandom.choice(possible_moves),\n",
    "\t\t\ttorch.tensor([0], device=device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransitionRecord:\n",
    "\tcurrent_state: Board\n",
    "\tnext_state: Board\n",
    "\timmediate_reward: torch.Tensor\n",
    "\n",
    "def optimize_model(memory: list[TransitionRecord]):\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\t\n",
    "\tstate_action_values = []\n",
    "\texpected_state_action_values = []\n",
    "\n",
    "\tfor r in random.sample(memory, BATCH_SIZE):\n",
    "\t\t# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "\t\t# columns of actions taken. These are the actions which would've been taken\n",
    "\t\t# for each batch state according to policy_net\n",
    "\t\tstate_action_values.append(max(q_s(policy_net, r.current_state), key=lambda x: x.value.item()).value)\n",
    "\n",
    "\t\t# Compute V(s_{t+1}) for all next states.\n",
    "\t\t# Expected values of actions for non_final_next_states are computed based\n",
    "\t\t# on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "\t\t# This is merged based on the mask, such that we'll have either the expected\n",
    "\t\t# state value or 0 in case the state was final.\n",
    "\t\tnext_state_value = 0\n",
    "\t\tif r.next_state.game_state == GameState.NOT_OVER:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tnext_state_value = max(q_s(target_net, r.next_state), key=lambda x: x.value.item()).value\n",
    "\t\t# Compute the expected Q values\n",
    "\t\texpected_state_action_values.append((next_state_value * GAMMA) + r.immediate_reward)\n",
    "\n",
    "\t# Compute Huber loss\n",
    "\tcriterion = nn.SmoothL1Loss()\n",
    "\tloss = criterion(\n",
    "\t\ttorch.cat(state_action_values),\n",
    "\t\ttorch.cat(expected_state_action_values)\n",
    "\t)\n",
    "\n",
    "\t# Optimize the model\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\t# In-place gradient clipping\n",
    "\ttorch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\toptimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(net: DQN) -> int:\n",
    "\tenemy = RandomPlayer(random.randint(0, 10000))\n",
    "\tboard = Board()\n",
    "\twhile board.game_state == GameState.NOT_OVER:\n",
    "\t\twhile board.game_state == GameState.NOT_OVER and board.turn_sign == 1:\n",
    "\t\t\tboard.make_move(*enemy.decide_move(board))\n",
    "\t\twhile board.game_state == GameState.NOT_OVER and board.turn_sign == -1:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tstate_values = max(q_s(net, board), key=lambda x: x.value.item())\n",
    "\t\t\t\tboard.make_move(*state_values.action)\n",
    "\t\n",
    "\tpieces = 0\n",
    "\tfor _, piece in board:\n",
    "\t\tpieces += piece != 0\n",
    "\n",
    "\treturn (1 if board.game_state == GameState(-1) else -1) * pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "backup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\user\\docs\\TUM\\ReinforcementLearning\\repos\\romaAI\\algo\\q_learning.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished. 1/20 | 9/20  12.0\n",
      "Episode 1 finished. 1/20 | 14/20  10.5\n",
      "Episode 2 finished. 1/20 | 11/20  9.666666666666666\n",
      "Episode 3 finished. 1/20 | 10/20  9.75\n",
      "Episode 4 finished. 1/20 | 11/20  9.4\n",
      "Episode 5 finished. 1/20 | 14/20  11.833333333333334\n",
      "Episode 6 finished. 1/20 | 12/20  11.714285714285714\n",
      "Episode 7 finished. 1/20 | 9/20  11.25\n",
      "Episode 8 finished. 1/20 | 13/20  11.11111111111111\n",
      "Episode 9 finished. 1/20 | 14/20  13.1\n",
      "Episode 10 finished. 1/20 | 11/20  12.636363636363637\n",
      "Episode 11 finished. 1/20 | 13/20  13.0\n",
      "Episode 12 finished. 1/20 | 12/20  12.846153846153847\n",
      "Episode 13 finished. 2/20 | 11/20  13.642857142857142\n",
      "Episode 14 finished. 3/20 | 13/20  14.266666666666667\n",
      "Episode 15 finished. 3/20 | 11/20  14.5\n",
      "Episode 16 finished. 3/20 | 9/20  14.647058823529411\n",
      "Episode 17 finished. 4/20 | 14/20  15.777777777777779\n",
      "Episode 18 finished. 4/20 | 10/20  15.473684210526315\n",
      "Episode 19 finished. 5/20 | 13/20  15.6\n",
      "Episode 20 finished. 4/20 | 11/20  16.80952380952381\n",
      "Episode 21 finished. 4/20 | 10/20  16.863636363636363\n",
      "Episode 22 finished. 4/20 | 14/20  16.565217391304348\n",
      "Episode 23 finished. 5/20 | 13/20  16.333333333333332\n",
      "Episode 24 finished. 5/20 | 9/20  16.4\n",
      "Episode 25 finished. 5/20 | 11/20  16.03846153846154\n",
      "Episode 26 finished. 5/20 | 10/20  16.22222222222222\n",
      "Episode 27 finished. 5/20 | 11/20  15.964285714285714\n",
      "Episode 28 finished. 6/20 | 10/20  15.689655172413794\n",
      "Episode 29 finished. 6/20 | 12/20  15.8\n",
      "Episode 30 finished. 7/20 | 11/20  15.64516129032258\n",
      "Episode 31 finished. 7/20 | 17/20  15.6875\n",
      "Episode 32 finished. 7/20 | 14/20  15.727272727272727\n",
      "Episode 33 finished. 7/20 | 14/20  15.529411764705882\n",
      "Episode 34 finished. 6/20 | 13/20  16.0\n",
      "Episode 35 finished. 7/20 | 11/20  16.0\n",
      "Episode 36 finished. 7/20 | 9/20  15.91891891891892\n",
      "Episode 37 finished. 7/20 | 11/20  16.13157894736842\n",
      "Episode 38 finished. 7/20 | 11/20  16.205128205128204\n",
      "Episode 39 finished. 6/20 | 13/20  16.55\n",
      "Episode 40 finished. 7/20 | 14/20  16.414634146341463\n",
      "Episode 41 finished. 7/20 | 11/20  16.285714285714285\n",
      "Episode 42 finished. 7/20 | 10/20  16.186046511627907\n",
      "Episode 43 finished. 7/20 | 10/20  16.068181818181817\n",
      "Episode 44 finished. 7/20 | 9/20  16.555555555555557\n",
      "Episode 45 finished. 8/20 | 11/20  16.58695652173913\n",
      "Episode 46 finished. 9/20 | 13/20  16.53191489361702\n",
      "Episode 47 finished. 9/20 | 14/20  16.416666666666668\n",
      "Episode 48 finished. 9/20 | 9/20  16.306122448979593\n",
      "Episode 49 finished. 9/20 | 13/20  16.14\n",
      "Episode 50 finished. 9/20 | 13/20  16.0\n",
      "Episode 51 finished. 9/20 | 15/20  16.01923076923077\n",
      "Episode 52 finished. 9/20 | 11/20  16.07547169811321\n",
      "Episode 53 finished. 8/20 | 9/20  16.40740740740741\n",
      "Episode 54 finished. 8/20 | 13/20  16.345454545454544\n",
      "Episode 55 finished. 7/20 | 11/20  17.089285714285715\n",
      "Episode 56 finished. 7/20 | 12/20  17.07017543859649\n",
      "Episode 57 finished. 7/20 | 13/20  17.051724137931036\n",
      "Episode 58 finished. 7/20 | 11/20  17.440677966101696\n",
      "Episode 59 finished. 8/20 | 9/20  17.6\n",
      "Episode 60 finished. 8/20 | 8/20  17.475409836065573\n",
      "Episode 61 finished. 8/20 | 13/20  17.661290322580644\n",
      "Episode 62 finished. 9/20 | 8/20  17.61904761904762\n",
      "Episode 63 finished. 8/20 | 13/20  17.5\n",
      "Episode 64 finished. 8/20 | 10/20  17.630769230769232\n",
      "Episode 65 finished. 7/20 | 8/20  17.575757575757574\n",
      "Episode 66 finished. 6/20 | 10/20  17.62686567164179\n",
      "Episode 67 finished. 6/20 | 6/20  17.529411764705884\n",
      "Episode 68 finished. 6/20 | 13/20  17.52173913043478\n",
      "Episode 69 finished. 6/20 | 9/20  17.485714285714284\n",
      "Episode 70 finished. 5/20 | 14/20  17.464788732394368\n",
      "Episode 71 finished. 5/20 | 14/20  17.52777777777778\n",
      "Episode 72 finished. 6/20 | 12/20  17.506849315068493\n",
      "Episode 73 finished. 7/20 | 10/20  17.594594594594593\n",
      "Episode 74 finished. 8/20 | 9/20  17.6\n",
      "Episode 75 finished. 8/20 | 11/20  17.57894736842105\n",
      "Episode 76 finished. 8/20 | 14/20  17.428571428571427\n",
      "Episode 77 finished. 8/20 | 10/20  17.71794871794872\n",
      "Episode 78 finished. 8/20 | 9/20  17.632911392405063\n",
      "Episode 79 finished. 7/20 | 14/20  17.7125\n",
      "Episode 80 finished. 6/20 | 9/20  17.641975308641975\n",
      "Episode 81 finished. 6/20 | 9/20  17.536585365853657\n",
      "Episode 82 finished. 5/20 | 11/20  17.481927710843372\n",
      "Episode 83 finished. 5/20 | 7/20  17.69047619047619\n",
      "Episode 84 finished. 6/20 | 14/20  17.658823529411766\n",
      "Episode 85 finished. 6/20 | 12/20  17.558139534883722\n",
      "Episode 86 finished. 7/20 | 11/20  17.50574712643678\n",
      "Episode 87 finished. 7/20 | 10/20  17.397727272727273\n",
      "Episode 88 finished. 6/20 | 12/20  17.314606741573034\n",
      "Episode 89 finished. 6/20 | 13/20  17.288888888888888\n",
      "Episode 90 finished. 6/20 | 14/20  17.186813186813186\n",
      "Episode 91 finished. 6/20 | 9/20  17.108695652173914\n",
      "Episode 92 finished. 5/20 | 13/20  17.021505376344088\n",
      "Episode 93 finished. 5/20 | 10/20  17.0\n",
      "Episode 94 finished. 4/20 | 15/20  16.91578947368421\n",
      "Episode 95 finished. 5/20 | 10/20  16.9375\n",
      "Episode 96 finished. 5/20 | 9/20  16.855670103092784\n",
      "Episode 97 finished. 4/20 | 11/20  16.8265306122449\n",
      "Episode 98 finished. 5/20 | 13/20  16.747474747474747\n",
      "Episode 99 finished. 5/20 | 15/20  16.67\n",
      "Episode 100 finished. 5/20 | 11/20  16.603960396039604\n",
      "Episode 101 finished. 5/20 | 12/20  16.862745098039216\n",
      "Episode 102 finished. 5/20 | 12/20  16.844660194174757\n",
      "Episode 103 finished. 5/20 | 12/20  17.057692307692307\n",
      "Episode 104 finished. 4/20 | 10/20  17.19047619047619\n",
      "Episode 105 finished. 5/20 | 13/20  17.245283018867923\n",
      "Episode 106 finished. 5/20 | 14/20  17.38317757009346\n",
      "Episode 107 finished. 6/20 | 11/20  17.324074074074073\n",
      "Episode 108 finished. 7/20 | 11/20  17.28440366972477\n",
      "Episode 109 finished. 7/20 | 13/20  17.37272727272727\n",
      "Episode 110 finished. 8/20 | 11/20  17.315315315315317\n",
      "Episode 111 finished. 8/20 | 11/20  17.241071428571427\n",
      "Episode 112 finished. 9/20 | 11/20  17.283185840707965\n",
      "Episode 113 finished. 8/20 | 13/20  17.614035087719298\n",
      "Episode 114 finished. 8/20 | 7/20  17.765217391304347\n",
      "Episode 115 finished. 7/20 | 10/20  17.913793103448278\n",
      "Episode 116 finished. 7/20 | 12/20  17.82905982905983\n",
      "Episode 117 finished. 7/20 | 10/20  17.89830508474576\n",
      "Episode 118 finished. 6/20 | 14/20  17.88235294117647\n",
      "Episode 119 finished. 6/20 | 9/20  17.908333333333335\n",
      "Episode 120 finished. 7/20 | 12/20  18.016528925619834\n",
      "Episode 121 finished. 7/20 | 7/20  18.139344262295083\n",
      "Episode 122 finished. 7/20 | 11/20  18.276422764227643\n",
      "Episode 123 finished. 7/20 | 13/20  18.427419354838708\n",
      "Episode 124 finished. 8/20 | 11/20  18.4\n",
      "Episode 125 finished. 7/20 | 13/20  18.325396825396826\n",
      "Episode 126 finished. 6/20 | 14/20  18.25984251968504\n",
      "Episode 127 finished. 6/20 | 15/20  18.1953125\n",
      "Episode 128 finished. 6/20 | 10/20  18.13953488372093\n",
      "Episode 129 finished. 6/20 | 11/20  18.06923076923077\n",
      "Episode 130 finished. 5/20 | 9/20  18.00763358778626\n",
      "Episode 131 finished. 5/20 | 12/20  17.946969696969695\n",
      "Episode 132 finished. 5/20 | 11/20  18.015037593984964\n",
      "Episode 133 finished. 5/20 | 13/20  17.955223880597014\n",
      "Episode 134 finished. 5/20 | 13/20  17.896296296296295\n",
      "Episode 135 finished. 5/20 | 14/20  18.022058823529413\n",
      "Episode 136 finished. 5/20 | 8/20  18.1021897810219\n",
      "Episode 137 finished. 5/20 | 12/20  18.333333333333332\n",
      "Episode 138 finished. 6/20 | 11/20  18.280575539568346\n",
      "Episode 139 finished. 6/20 | 12/20  18.27857142857143\n",
      "Episode 140 finished. 6/20 | 12/20  18.29787234042553\n",
      "Episode 141 finished. 7/20 | 14/20  18.295774647887324\n",
      "Episode 142 finished. 7/20 | 15/20  18.384615384615383\n",
      "Episode 143 finished. 8/20 | 9/20  18.354166666666668\n",
      "Episode 144 finished. 7/20 | 14/20  18.30344827586207\n",
      "Episode 145 finished. 8/20 | 11/20  18.23972602739726\n",
      "Episode 146 finished. 8/20 | 10/20  18.183673469387756\n",
      "Episode 147 finished. 8/20 | 12/20  18.12837837837838\n",
      "Episode 148 finished. 7/20 | 10/20  18.073825503355703\n",
      "Episode 149 finished. 7/20 | 12/20  18.013333333333332\n",
      "Episode 150 finished. 7/20 | 12/20  18.158940397350992\n",
      "Episode 151 finished. 7/20 | 10/20  18.098684210526315\n",
      "Episode 152 finished. 6/20 | 10/20  18.03921568627451\n",
      "Episode 153 finished. 6/20 | 10/20  18.032467532467532\n",
      "Episode 154 finished. 7/20 | 6/20  18.045161290322582\n",
      "Episode 155 finished. 7/20 | 8/20  18.141025641025642\n",
      "Episode 156 finished. 7/20 | 12/20  18.235668789808916\n",
      "Episode 157 finished. 7/20 | 9/20  18.246835443037973\n",
      "Episode 158 finished. 6/20 | 11/20  18.169811320754718\n",
      "Episode 159 finished. 6/20 | 14/20  18.275\n",
      "Episode 160 finished. 6/20 | 14/20  18.335403726708076\n",
      "Episode 161 finished. 5/20 | 16/20  18.320987654320987\n",
      "Episode 162 finished. 5/20 | 11/20  18.39877300613497\n",
      "Episode 163 finished. 4/20 | 17/20  18.475609756097562\n",
      "Episode 164 finished. 5/20 | 8/20  18.44242424242424\n",
      "Episode 165 finished. 4/20 | 12/20  18.518072289156628\n",
      "Episode 166 finished. 4/20 | 11/20  18.592814371257486\n",
      "Episode 167 finished. 3/20 | 10/20  18.666666666666668\n",
      "Episode 168 finished. 4/20 | 8/20  18.72189349112426\n",
      "Episode 169 finished. 4/20 | 10/20  18.694117647058825\n",
      "Episode 170 finished. 4/20 | 11/20  18.678362573099417\n",
      "Episode 171 finished. 5/20 | 9/20  18.63953488372093\n",
      "Episode 172 finished. 5/20 | 9/20  18.722543352601157\n",
      "Episode 173 finished. 6/20 | 11/20  18.67241379310345\n",
      "Episode 174 finished. 5/20 | 11/20  18.611428571428572\n",
      "Episode 175 finished. 5/20 | 9/20  18.551136363636363\n",
      "Episode 176 finished. 6/20 | 9/20  18.581920903954803\n",
      "Episode 177 finished. 7/20 | 9/20  18.573033707865168\n",
      "Episode 178 finished. 8/20 | 13/20  18.53072625698324\n",
      "Episode 179 finished. 8/20 | 10/20  18.516666666666666\n",
      "Episode 180 finished. 7/20 | 9/20  18.46961325966851\n",
      "Episode 181 finished. 7/20 | 10/20  18.412087912087912\n",
      "Episode 182 finished. 7/20 | 14/20  18.508196721311474\n",
      "Episode 183 finished. 7/20 | 14/20  18.60326086956522\n",
      "Episode 184 finished. 6/20 | 11/20  18.697297297297297\n",
      "Episode 185 finished. 6/20 | 15/20  18.693548387096776\n",
      "Episode 186 finished. 6/20 | 8/20  18.684491978609625\n",
      "Episode 187 finished. 6/20 | 12/20  18.77659574468085\n",
      "Episode 188 finished. 6/20 | 8/20  18.78306878306878\n",
      "Episode 189 finished. 6/20 | 13/20  18.857894736842105\n",
      "Episode 190 finished. 6/20 | 13/20  18.790575916230367\n",
      "Episode 191 finished. 5/20 | 10/20  18.880208333333332\n",
      "Episode 192 finished. 5/20 | 9/20  18.83937823834197\n",
      "Episode 193 finished. 5/20 | 11/20  18.835051546391753\n",
      "Episode 194 finished. 5/20 | 9/20  18.923076923076923\n",
      "Episode 195 finished. 5/20 | 8/20  18.9234693877551\n",
      "Episode 196 finished. 4/20 | 13/20  19.01015228426396\n",
      "Episode 197 finished. 4/20 | 12/20  18.964646464646464\n",
      "Episode 198 finished. 4/20 | 10/20  18.964824120603016\n",
      "Episode 199 finished. 5/20 | 12/20  18.96\n",
      "Episode 200 finished. 6/20 | 9/20  18.955223880597014\n",
      "Episode 201 finished. 7/20 | 12/20  18.965346534653467\n",
      "Episode 202 finished. 8/20 | 10/20  19.014778325123153\n",
      "Episode 203 finished. 8/20 | 13/20  18.995098039215687\n",
      "Episode 204 finished. 8/20 | 12/20  18.960975609756098\n",
      "Episode 205 finished. 8/20 | 10/20  19.131067961165048\n",
      "Episode 206 finished. 9/20 | 14/20  19.096618357487923\n",
      "Episode 207 finished. 9/20 | 12/20  19.048076923076923\n",
      "Episode 208 finished. 8/20 | 8/20  18.985645933014354\n",
      "Episode 209 finished. 8/20 | 10/20  18.97142857142857\n",
      "Episode 210 finished. 8/20 | 12/20  18.933649289099527\n",
      "Episode 211 finished. 8/20 | 11/20  18.919811320754718\n",
      "Episode 212 finished. 9/20 | 12/20  18.906103286384976\n",
      "Episode 213 finished. 8/20 | 12/20  18.89252336448598\n",
      "Episode 214 finished. 8/20 | 12/20  18.87906976744186\n",
      "Episode 215 finished. 9/20 | 11/20  18.912037037037038\n",
      "Episode 216 finished. 9/20 | 13/20  18.86635944700461\n",
      "Episode 217 finished. 9/20 | 10/20  18.839449541284402\n",
      "Episode 218 finished. 8/20 | 8/20  18.995433789954337\n",
      "Episode 219 finished. 7/20 | 7/20  19.095454545454544\n",
      "Episode 220 finished. 6/20 | 16/20  19.171945701357465\n",
      "Episode 221 finished. 5/20 | 10/20  19.153153153153152\n",
      "Episode 222 finished. 5/20 | 10/20  19.116591928251122\n",
      "Episode 223 finished. 5/20 | 11/20  19.1875\n",
      "Episode 224 finished. 6/20 | 12/20  19.24\n",
      "Episode 225 finished. 6/20 | 8/20  19.278761061946902\n",
      "Episode 226 finished. 5/20 | 11/20  19.26872246696035\n",
      "Episode 227 finished. 6/20 | 11/20  19.289473684210527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m target_net\u001b[38;5;241m.\u001b[39mload_state_dict(target_net_state_dict)\n\u001b[0;32m     43\u001b[0m number_of_steps\u001b[38;5;241m.\u001b[39mappend(num_steps)\n\u001b[1;32m---> 44\u001b[0m stats \u001b[38;5;241m=\u001b[39m [run_game(target_net) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[0;32m     45\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(win_rate[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:])\n\u001b[0;32m     46\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stats)\n",
      "Cell \u001b[1;32mIn[63], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m target_net\u001b[38;5;241m.\u001b[39mload_state_dict(target_net_state_dict)\n\u001b[0;32m     43\u001b[0m number_of_steps\u001b[38;5;241m.\u001b[39mappend(num_steps)\n\u001b[1;32m---> 44\u001b[0m stats \u001b[38;5;241m=\u001b[39m [\u001b[43mrun_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[0;32m     45\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(win_rate[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:])\n\u001b[0;32m     46\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stats)\n",
      "Cell \u001b[1;32mIn[61], line 9\u001b[0m, in \u001b[0;36mrun_game\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m      7\u001b[0m \t\u001b[38;5;28;01mwhile\u001b[39;00m board\u001b[38;5;241m.\u001b[39mgame_state \u001b[38;5;241m==\u001b[39m GameState\u001b[38;5;241m.\u001b[39mNOT_OVER \u001b[38;5;129;01mand\u001b[39;00m board\u001b[38;5;241m.\u001b[39mturn_sign \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      8\u001b[0m \t\t\u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 9\u001b[0m \t\t\tstate_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mq_s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     10\u001b[0m \t\t\tboard\u001b[38;5;241m.\u001b[39mmake_move(\u001b[38;5;241m*\u001b[39mstate_values\u001b[38;5;241m.\u001b[39maction)\n\u001b[0;32m     12\u001b[0m pieces \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[58], line 17\u001b[0m, in \u001b[0;36mq_s\u001b[1;34m(dqn, current_state)\u001b[0m\n\u001b[0;32m     15\u001b[0m \t\tnext_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(current_state)\n\u001b[0;32m     16\u001b[0m \t\timmediate_reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([next_state\u001b[38;5;241m.\u001b[39mmake_move(s, e) \u001b[38;5;241m*\u001b[39m next_state\u001b[38;5;241m.\u001b[39mturn_sign], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 17\u001b[0m \t\tvalue \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m GAMMA \u001b[38;5;241m+\u001b[39m immediate_reward\n\u001b[0;32m     18\u001b[0m \t\tret\u001b[38;5;241m.\u001b[39mappend(Action((s, e), value))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\user\\docs\\TUM\\ReinforcementLearning\\repos\\romaAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\user\\docs\\TUM\\ReinforcementLearning\\repos\\romaAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[56], line 36\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, board)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, board: Board) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 36\u001b[0m \tstate \u001b[38;5;241m=\u001b[39m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     38\u001b[0m \t\tstate \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(layer(state))\n",
      "File \u001b[1;32mc:\\user\\docs\\TUM\\ReinforcementLearning\\repos\\romaAI\\algo\\board.py:415\u001b[0m, in \u001b[0;36mBoard.to_tensor\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y), piece \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    413\u001b[0m \t\u001b[38;5;66;03m#       possition coding                   piece coding\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \tindex \u001b[38;5;241m=\u001b[39m ((x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSIZE \u001b[38;5;241m*\u001b[39m y) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m (piece \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 415\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tensor_cache[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tensor_cache\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "\tnum_episodes = 600\n",
    "else:\n",
    "\tnum_episodes = 50\n",
    "\t\n",
    "our_sign = -1\n",
    "memory: list[TransitionRecord] = []\n",
    "\n",
    "win_rate = []\n",
    "number_of_steps = []\n",
    "\n",
    "enemy = RandomPlayer(random.randint(0, 1000))\n",
    "q_enemy = QLearning(\"dqn80.pth\")\n",
    "random_enemy = RandomPlayer(random.randint(0, 1000))\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\tenemy = random.choice([q_enemy, random_enemy])\n",
    "\tcur_state = Board()\n",
    "\tcur_state.make_move(*enemy.decide_move(cur_state))\n",
    "\tnum_steps = 0\n",
    "\twhile True:\n",
    "\t\tnum_steps += 1\n",
    "\t\taction = select_action(cur_state)\n",
    "\t\tnew_state, immediate_reward = make_environment_step(cur_state, action.action, enemy, num_steps)\n",
    "\n",
    "\t\tmemory.append(TransitionRecord(cur_state, new_state, torch.tensor([immediate_reward], device=device)))\n",
    "\t\tcur_state = new_state\n",
    "\n",
    "\t\tif cur_state.game_state != GameState.NOT_OVER:\n",
    "\t\t\twin_rate.append(cur_state.game_state == GameState(our_sign))\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Perform one step of the optimization (on the policy network)\n",
    "\toptimize_model(memory)\n",
    "\t# Soft update of the target network's weights\n",
    "\t# θ′ ← τ θ + (1 −τ )θ′\n",
    "\ttarget_net_state_dict = target_net.state_dict()\n",
    "\tpolicy_net_state_dict = policy_net.state_dict()\n",
    "\tfor key in policy_net_state_dict:\n",
    "\t\ttarget_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\ttarget_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\tnumber_of_steps.append(num_steps)\n",
    "\tstats = [run_game(target_net) for _ in range(20)]\n",
    "\ta = sum(win_rate[-20:])\n",
    "\tb = sum(x > 0 for x in stats)\n",
    "\n",
    "\tif a * b > max_score:\n",
    "\t\tmax_score = a * b\n",
    "\t\tbackup = copy.deepcopy(target_net)\n",
    "\n",
    "\tprint(f\"Episode {i_episode} finished. {a}/{20} | {b}/20  {sum(number_of_steps)/len(number_of_steps)}\")\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966666666666667"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(win_rate) / len(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(323)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(551), 1000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = [run_game(target_net) for _ in range(1000)]\n",
    "\n",
    "plt.hist(stats)\n",
    "sum(x > 0 for x in stats), len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(570), 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert backup\n",
    "stats = [run_game(backup) for _ in range(1000)]\n",
    "\n",
    "plt.hist(stats)\n",
    "sum(x > 0 for x in stats), len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(policy_net.state_dict(), \"~dqn83 90 50 20 1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_net.load_state_dict(torch.load(\"dqn.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
