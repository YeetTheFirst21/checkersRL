{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo.dynamicProgramming import dynamicPlayer\n",
    "from algo.iplayer import RandomPlayer, IPlayer\n",
    "from algo.q_learning import QLearning\n",
    "from algo.board import Board, GameState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    # \"cuda\" if torch.cuda.is_available() else\n",
    "    # \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tUsing structure similar to NNUE:\n",
    "\thttps://www.chessprogramming.org/File:StockfishNNUELayers.png\n",
    "\t\n",
    "\tObservation space: hot-encoded board:\n",
    "\tfor each of 18 cells we can be -2, -1, 0, 1, 2 (5 possibilities).\n",
    "\tIn total it gives 18 * 5 = 90 possible inputs, out of which at most 12 are on.\n",
    "\t\n",
    "\t# Action space: 4 possible actions.\n",
    "\tValue function: 1 output. # https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DQN, self).__init__()\n",
    "\n",
    "\t\tlayer_sizes = [\n",
    "\t\t\t90,\n",
    "\t\t\t50,\n",
    "\t\t\t50,\n",
    "\t\t\t1\n",
    "\t\t]\n",
    "\n",
    "\t\tlayers = []\n",
    "\t\tprev_size = layer_sizes[0]\n",
    "\t\tfor cur_size in layer_sizes[1:]:\n",
    "\t\t\tlayers.append(nn.Linear(prev_size, cur_size))\n",
    "\t\t\tprev_size = cur_size\n",
    "\n",
    "\t\tself.layers = nn.ModuleList(layers)\n",
    "\n",
    "\tdef forward(self, board: Board, flipped = False) -> torch.Tensor:\n",
    "\t\tstate = board.to_tensor(device, flipped)\n",
    "\t\tfor layer in self.layers[:-1]:\n",
    "\t\t\tstate = F.relu(layer(state))\n",
    "\t\treturn self.layers[-1](state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment_step(state: Board, action: tuple[tuple[int, int], tuple[int, int]], enemy: IPlayer, current_step: int) -> tuple[Board, torch.Tensor]:\n",
    "\t\"\"\"\n",
    "\tReturns new state and reward for the given action.\n",
    "\t\"\"\"\n",
    "\tstate = copy.deepcopy(state)\n",
    "\tcur_sign = state.turn_sign\n",
    "\twe_captured = state.make_move(*action) * cur_sign\n",
    "\tenemy_captured = 0\n",
    "\twhile state.game_state == GameState.NOT_OVER and state.turn_sign != cur_sign:\n",
    "\t\tenemy_captured += state.make_move(*enemy.decide_move(state)) * cur_sign * (-1)\n",
    "\n",
    "\treward = we_captured - enemy_captured\n",
    "\tif current_step > 10 and not we_captured:\n",
    "\t\treward -= 2\n",
    "\t\t\n",
    "\tif state.game_state != GameState.NOT_OVER:\n",
    "\t\tour_pieces = 0\n",
    "\t\tenemy_pieces = 0\n",
    "\t\tfor _, piece in state:\n",
    "\t\t\tif piece == cur_sign:\n",
    "\t\t\t\tour_pieces += 1\n",
    "\t\t\telif piece == -cur_sign:\n",
    "\t\t\t\tenemy_pieces += 1\n",
    "\t\t\telif piece == 2 * cur_sign:\n",
    "\t\t\t\tour_pieces += 2\n",
    "\t\t\telif piece == -2 * cur_sign:\n",
    "\t\t\t\tenemy_pieces += 2\n",
    "\t\t\t\n",
    "\t\treward += 3 * our_pieces / (enemy_pieces + 1)\n",
    "\n",
    "\t\tif state.game_state == GameState.DRAW:\n",
    "\t\t\treward -= 40\n",
    "\t\telif state.game_state == GameState(cur_sign):\n",
    "\t\t\treward += 40\n",
    "\t\telif state.game_state == GameState(-cur_sign):\n",
    "\t\t\treward -= 40\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Unexpected game state\")\n",
    "\t\n",
    "\treturn state, torch.Tensor([reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def move_iterator(state: Board) -> Iterator[tuple[tuple[int, int], tuple[int, int]]]:\n",
    "\tfor s in state.get_possible_pos():\n",
    "\t\tfor e in state.get_possible_pos():\n",
    "\t\t\tyield (s, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99 # discount rate\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "\taction: tuple[tuple[int, int], tuple[int, int]]\n",
    "\tvalue: torch.Tensor\n",
    "\n",
    "def q_s(dqn: DQN, current_state: Board, our_sign: int, flipped = False) -> Action:\n",
    "\t\"\"\"\n",
    "\tReturn: list[(new_state, action, immediate_reward, value)]\n",
    "\t\"\"\"\n",
    "\tret: list[Action] = []\n",
    "\tfor s in current_state.get_possible_pos():\n",
    "\t\tfor e in current_state.get_correct_moves(s):\n",
    "\t\t\tnext_state = copy.deepcopy(current_state)\n",
    "\t\t\timmediate_reward = torch.tensor([next_state.make_move(s, e) * our_sign], device=device)\n",
    "\t\t\tvalue = dqn(next_state, flipped) * GAMMA + immediate_reward\n",
    "\t\t\tret.append(Action((s, e), value))\n",
    "\t\n",
    "\treturn max(ret, key=lambda x: x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td2(dqn: DQN, current_state: Board) -> Action:\n",
    "\tcur_sign = current_state.turn_sign\n",
    "\tret: list[Action] = []\n",
    "\tfor my_move in move_iterator(current_state):\n",
    "\t\tnext_state = copy.deepcopy(current_state)\n",
    "\t\tvalue = torch.tensor([next_state.make_move(*my_move) * next_state.turn_sign], device=device)\n",
    "\n",
    "\t\tstep = 1\n",
    "\t\twhile next_state.game_state == GameState.NOT_OVER and next_state.turn_sign == cur_sign:\n",
    "\t\t\tvalue += next_state.make_move(*q_s(dqn, next_state, cur_sign).action) * cur_sign * GAMMA ** step\n",
    "\t\t\tstep += 1\n",
    "\t\t\n",
    "\t\twhile next_state.game_state == GameState.NOT_OVER and next_state.turn_sign != cur_sign:\n",
    "\t\t\ts, e = q_s(dqn, next_state, -cur_sign, True).action\n",
    "\t\t\tvalue -= next_state.make_move((s[1], s[0]), (e[1], e[0])) * cur_sign * GAMMA ** step\n",
    "\t\t\tstep += 1\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_304692\\634578881.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(\"dqn80.pth\"))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 # number of transitions sampled from the replay buffer\n",
    "\n",
    "EPS_START = 0.05 # exploration rate\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005 # update rate\n",
    "LR = 1e-4 # AdamW learning rate\n",
    "\n",
    "policy_net = DQN().to(device) # to be updated often\n",
    "target_net = DQN().to(device) # to be updated with TAU\n",
    "policy_net.load_state_dict(torch.load(\"dqn80.pth\"))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(board: Board) -> Action:\n",
    "\tglobal steps_done\n",
    "\tsample = random.random()\n",
    "\teps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "\t\tmath.exp(-1. * steps_done / EPS_DECAY)\n",
    "\tsteps_done += 1\n",
    "\tif sample > eps_threshold:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate_values = q_s(policy_net, board)\n",
    "\t\t\treturn max(state_values, key=lambda x: x.value.item())\n",
    "\t\t\t\t\t\n",
    "\telse:\n",
    "\t\tpossible_moves = []\n",
    "\t\tfor s in board.get_possible_pos():\n",
    "\t\t\tfor e in board.get_correct_moves(s):\n",
    "\t\t\t\tpossible_moves.append((s, e))\n",
    "\t\treturn Action(\n",
    "\t\t\trandom.choice(possible_moves),\n",
    "\t\t\ttorch.tensor([0], device=device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransitionRecord:\n",
    "\tcurrent_state: Board\n",
    "\tnext_state: Board\n",
    "\timmediate_reward: torch.Tensor\n",
    "\n",
    "def optimize_model(memory: list[TransitionRecord]):\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\t\n",
    "\tstate_action_values = []\n",
    "\texpected_state_action_values = []\n",
    "\n",
    "\tfor r in random.sample(memory, BATCH_SIZE):\n",
    "\t\t# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "\t\t# columns of actions taken. These are the actions which would've been taken\n",
    "\t\t# for each batch state according to policy_net\n",
    "\t\tstate_action_values.append(max(q_s(policy_net, r.current_state), key=lambda x: x.value.item()).value)\n",
    "\n",
    "\t\t# Compute V(s_{t+1}) for all next states.\n",
    "\t\t# Expected values of actions for non_final_next_states are computed based\n",
    "\t\t# on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "\t\t# This is merged based on the mask, such that we'll have either the expected\n",
    "\t\t# state value or 0 in case the state was final.\n",
    "\t\tnext_state_value = 0\n",
    "\t\tif r.next_state.game_state == GameState.NOT_OVER:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tnext_state_value = max(q_s(target_net, r.next_state), key=lambda x: x.value.item()).value\n",
    "\t\t# Compute the expected Q values\n",
    "\t\texpected_state_action_values.append((next_state_value * GAMMA) + r.immediate_reward)\n",
    "\n",
    "\t# Compute Huber loss\n",
    "\tcriterion = nn.SmoothL1Loss()\n",
    "\tloss = criterion(\n",
    "\t\ttorch.cat(state_action_values),\n",
    "\t\ttorch.cat(expected_state_action_values)\n",
    "\t)\n",
    "\n",
    "\t# Optimize the model\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\t# In-place gradient clipping\n",
    "\ttorch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\toptimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(net: DQN) -> int:\n",
    "\tenemy = RandomPlayer(random.randint(0, 10000))\n",
    "\tboard = Board()\n",
    "\twhile board.game_state == GameState.NOT_OVER:\n",
    "\t\twhile board.game_state == GameState.NOT_OVER and board.turn_sign == 1:\n",
    "\t\t\tboard.make_move(*enemy.decide_move(board))\n",
    "\t\twhile board.game_state == GameState.NOT_OVER and board.turn_sign == -1:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tstate_values = max(q_s(net, board), key=lambda x: x.value.item())\n",
    "\t\t\t\tboard.make_move(*state_values.action)\n",
    "\t\n",
    "\tpieces = 0\n",
    "\tfor _, piece in board:\n",
    "\t\tpieces += piece != 0\n",
    "\n",
    "\treturn (1 if board.game_state == GameState(-1) else -1) * pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "backup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\user\\docs\\TUM\\ReinforcementLearning\\repos\\romaAI\\algo\\q_learning.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished. 0/20 | 14/20  37.0\n",
      "Episode 1 finished. 1/20 | 17/20  28.0\n",
      "Episode 2 finished. 2/20 | 16/20  22.333333333333332\n",
      "Episode 3 finished. 3/20 | 15/20  19.0\n",
      "Episode 4 finished. 4/20 | 17/20  19.8\n",
      "Episode 5 finished. 5/20 | 18/20  18.666666666666668\n",
      "Episode 6 finished. 6/20 | 17/20  17.285714285714285\n",
      "Episode 7 finished. 6/20 | 18/20  19.75\n",
      "Episode 8 finished. 7/20 | 14/20  18.555555555555557\n",
      "Episode 9 finished. 8/20 | 18/20  17.6\n",
      "Episode 10 finished. 9/20 | 16/20  17.0\n",
      "Episode 11 finished. 10/20 | 16/20  17.25\n",
      "Episode 12 finished. 11/20 | 16/20  16.76923076923077\n",
      "Episode 13 finished. 11/20 | 17/20  17.428571428571427\n",
      "Episode 14 finished. 12/20 | 17/20  17.133333333333333\n",
      "Episode 15 finished. 13/20 | 13/20  16.6875\n",
      "Episode 16 finished. 14/20 | 12/20  16.294117647058822\n",
      "Episode 17 finished. 15/20 | 15/20  15.944444444444445\n",
      "Episode 18 finished. 16/20 | 16/20  15.578947368421053\n",
      "Episode 19 finished. 17/20 | 17/20  15.65\n",
      "Episode 20 finished. 17/20 | 16/20  15.761904761904763\n",
      "Episode 21 finished. 17/20 | 14/20  15.727272727272727\n",
      "Episode 22 finished. 16/20 | 15/20  16.26086956521739\n",
      "Episode 23 finished. 16/20 | 13/20  16.0\n",
      "Episode 24 finished. 16/20 | 16/20  15.96\n",
      "Episode 25 finished. 16/20 | 15/20  15.807692307692308\n",
      "Episode 26 finished. 15/20 | 19/20  16.555555555555557\n",
      "Episode 27 finished. 16/20 | 19/20  16.642857142857142\n",
      "Episode 28 finished. 16/20 | 11/20  16.413793103448278\n",
      "Episode 29 finished. 16/20 | 14/20  16.233333333333334\n",
      "Episode 30 finished. 16/20 | 15/20  16.096774193548388\n",
      "Episode 31 finished. 16/20 | 15/20  15.875\n",
      "Episode 32 finished. 16/20 | 16/20  15.757575757575758\n",
      "Episode 33 finished. 16/20 | 16/20  16.294117647058822\n",
      "Episode 34 finished. 16/20 | 15/20  16.085714285714285\n",
      "Episode 35 finished. 16/20 | 16/20  16.305555555555557\n",
      "Episode 36 finished. 15/20 | 17/20  16.10810810810811\n",
      "Episode 37 finished. 15/20 | 15/20  15.921052631578947\n",
      "Episode 38 finished. 15/20 | 18/20  15.743589743589743\n",
      "Episode 39 finished. 15/20 | 17/20  15.9\n",
      "Episode 40 finished. 16/20 | 14/20  16.121951219512194\n",
      "Episode 41 finished. 16/20 | 13/20  15.976190476190476\n",
      "Episode 42 finished. 17/20 | 14/20  16.232558139534884\n",
      "Episode 43 finished. 17/20 | 17/20  16.34090909090909\n",
      "Episode 44 finished. 17/20 | 15/20  16.2\n",
      "Episode 45 finished. 17/20 | 16/20  16.347826086956523\n",
      "Episode 46 finished. 18/20 | 14/20  16.27659574468085\n",
      "Episode 47 finished. 18/20 | 13/20  16.3125\n",
      "Episode 48 finished. 18/20 | 17/20  16.20408163265306\n",
      "Episode 49 finished. 18/20 | 13/20  16.42\n",
      "Episode 50 finished. 18/20 | 15/20  16.254901960784313\n",
      "Episode 51 finished. 17/20 | 15/20  16.134615384615383\n",
      "Episode 52 finished. 17/20 | 13/20  16.0188679245283\n",
      "Episode 53 finished. 18/20 | 15/20  16.11111111111111\n",
      "Episode 54 finished. 18/20 | 13/20  16.09090909090909\n",
      "Episode 55 finished. 18/20 | 15/20  15.982142857142858\n",
      "Episode 56 finished. 18/20 | 16/20  16.105263157894736\n",
      "Episode 57 finished. 18/20 | 13/20  16.0\n",
      "Episode 58 finished. 18/20 | 16/20  16.084745762711865\n",
      "Episode 59 finished. 18/20 | 16/20  16.266666666666666\n",
      "Episode 60 finished. 18/20 | 17/20  16.24590163934426\n",
      "Episode 61 finished. 18/20 | 14/20  16.193548387096776\n",
      "Episode 62 finished. 18/20 | 17/20  16.142857142857142\n",
      "Episode 63 finished. 18/20 | 16/20  16.296875\n",
      "Episode 64 finished. 18/20 | 16/20  16.2\n",
      "Episode 65 finished. 18/20 | 16/20  16.40909090909091\n",
      "Episode 66 finished. 18/20 | 14/20  16.73134328358209\n",
      "Episode 67 finished. 17/20 | 14/20  16.61764705882353\n",
      "Episode 68 finished. 17/20 | 14/20  16.63768115942029\n",
      "Episode 69 finished. 17/20 | 12/20  16.857142857142858\n",
      "Episode 70 finished. 17/20 | 16/20  16.95774647887324\n",
      "Episode 71 finished. 17/20 | 17/20  16.833333333333332\n",
      "Episode 72 finished. 17/20 | 16/20  16.986301369863014\n",
      "Episode 73 finished. 17/20 | 16/20  16.89189189189189\n",
      "Episode 74 finished. 16/20 | 13/20  17.12\n",
      "Episode 75 finished. 15/20 | 20/20  17.18421052631579\n",
      "Episode 76 finished. 16/20 | 15/20  17.194805194805195\n",
      "Episode 77 finished. 16/20 | 17/20  17.192307692307693\n",
      "Episode 78 finished. 15/20 | 15/20  17.265822784810126\n",
      "Episode 79 finished. 15/20 | 13/20  17.5\n",
      "Episode 80 finished. 14/20 | 15/20  17.395061728395063\n",
      "Episode 81 finished. 14/20 | 17/20  17.317073170731707\n",
      "Episode 82 finished. 14/20 | 18/20  17.253012048192772\n",
      "Episode 83 finished. 13/20 | 14/20  17.416666666666668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \t\t\u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     40\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m(memory)\u001b[0m\n\u001b[0;32m     12\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m random\u001b[38;5;241m.\u001b[39msample(memory, BATCH_SIZE):\n\u001b[0;32m     15\u001b[0m \t\u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \t\u001b[38;5;66;03m# columns of actions taken. These are the actions which would've been taken\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \t\u001b[38;5;66;03m# for each batch state according to policy_net\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \tstate_action_values\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq_s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m     20\u001b[0m \t\u001b[38;5;66;03m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \t\u001b[38;5;66;03m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \t\u001b[38;5;66;03m# on the \"older\" target_net; selecting their best reward with max(1).values\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \t\u001b[38;5;66;03m# This is merged based on the mask, such that we'll have either the expected\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \t\u001b[38;5;66;03m# state value or 0 in case the state was final.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \tnext_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "\tnum_episodes = 600\n",
    "else:\n",
    "\tnum_episodes = 50\n",
    "\t\n",
    "our_sign = -1\n",
    "memory: list[TransitionRecord] = []\n",
    "\n",
    "win_rate = []\n",
    "number_of_steps = []\n",
    "\n",
    "# enemy = RandomPlayer(random.randint(0, 1000))\n",
    "q_enemy = QLearning(\"dqn80.pth\")\n",
    "random_enemy = RandomPlayer(random.randint(0, 1000))\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\tfor enemy in [random_enemy]:\n",
    "\t\tcur_state = Board()\n",
    "\t\tcur_state.make_move(*enemy.decide_move(cur_state))\n",
    "\t\tnum_steps = 0\n",
    "\t\twhile True:\n",
    "\t\t\tnum_steps += 1\n",
    "\t\t\taction = select_action(cur_state)\n",
    "\t\t\tnew_state, immediate_reward = make_environment_step(cur_state, action.action, enemy, num_steps)\n",
    "\n",
    "\t\t\tmemory.append(TransitionRecord(cur_state, new_state, torch.tensor([immediate_reward], device=device)))\n",
    "\t\t\tcur_state = new_state\n",
    "\n",
    "\t\t\tif cur_state.game_state != GameState.NOT_OVER:\n",
    "\t\t\t\twin_rate.append(cur_state.game_state == GameState(our_sign))\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# Perform one step of the optimization (on the policy network)\n",
    "\t\toptimize_model(memory)\n",
    "\t\t# Soft update of the target network's weights\n",
    "\t\t# θ′ ← τ θ + (1 −τ )θ′\n",
    "\t\ttarget_net_state_dict = target_net.state_dict()\n",
    "\t\tpolicy_net_state_dict = policy_net.state_dict()\n",
    "\t\tfor key in policy_net_state_dict:\n",
    "\t\t\ttarget_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\t\ttarget_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\tnumber_of_steps.append(num_steps)\n",
    "\tstats = [run_game(target_net) for _ in range(20)]\n",
    "\ta = sum(win_rate[-20:])\n",
    "\tb = sum(x > 0 for x in stats)\n",
    "\n",
    "\tif a * b > max_score:\n",
    "\t\tmax_score = a * b\n",
    "\t\tbackup = copy.deepcopy(target_net)\n",
    "\n",
    "\tprint(f\"Episode {i_episode} finished. {a}/{20} | {b}/20  {sum(number_of_steps)/len(number_of_steps)}\")\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966666666666667"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(win_rate) / len(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(112)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [run_game(target_net) for _ in range(1000)]\n",
    "\n",
    "plt.hist(stats)\n",
    "sum(x > 0 for x in stats), len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(642), 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert backup\n",
    "stats = [run_game(backup) for _ in range(1000)]\n",
    "\n",
    "plt.hist(stats)\n",
    "sum(x > 0 for x in stats), len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"~dqn83 90 50 20 1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Local\\Temp\\ipykernel_268456\\1728794949.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_net.load_state_dict(torch.load(\"dqn.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net.load_state_dict(torch.load(\"dqn.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
